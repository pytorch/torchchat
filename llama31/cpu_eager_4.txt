
python3 torchchat.py generate llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --prompt "Once upon a time," --max-new-tokens 256 --num-samples 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py generate llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --prompt "Once upon a time," --max-new-tokens 256 --num-samples 3
2024-10-31:18:56:21,889 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wq, in=4096, out=4096
2024-10-31:18:56:22,223 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wk, in=4096, out=1024
2024-10-31:18:56:22,363 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wv, in=4096, out=1024
2024-10-31:18:56:22,507 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wo, in=4096, out=4096
2024-10-31:18:56:22,830 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:23,241 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:23,669 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:24,073 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wq, in=4096, out=4096
2024-10-31:18:56:24,402 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wk, in=4096, out=1024
2024-10-31:18:56:24,542 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wv, in=4096, out=1024
2024-10-31:18:56:24,676 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wo, in=4096, out=4096
2024-10-31:18:56:24,991 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:25,413 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:25,842 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:26,255 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wq, in=4096, out=4096
2024-10-31:18:56:26,568 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wk, in=4096, out=1024
2024-10-31:18:56:26,705 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wv, in=4096, out=1024
2024-10-31:18:56:26,845 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wo, in=4096, out=4096
2024-10-31:18:56:27,152 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:27,545 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:27,955 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:28,395 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wq, in=4096, out=4096
2024-10-31:18:56:28,725 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wk, in=4096, out=1024
2024-10-31:18:56:28,854 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wv, in=4096, out=1024
2024-10-31:18:56:28,983 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wo, in=4096, out=4096
2024-10-31:18:56:29,295 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:29,718 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:30,128 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:30,539 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wq, in=4096, out=4096
2024-10-31:18:56:30,855 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wk, in=4096, out=1024
2024-10-31:18:56:30,993 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wv, in=4096, out=1024
2024-10-31:18:56:31,126 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wo, in=4096, out=4096
2024-10-31:18:56:31,454 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:31,868 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:32,283 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:32,701 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wq, in=4096, out=4096
2024-10-31:18:56:33,018 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wk, in=4096, out=1024
2024-10-31:18:56:33,149 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wv, in=4096, out=1024
2024-10-31:18:56:33,282 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wo, in=4096, out=4096
2024-10-31:18:56:33,598 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:34,024 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:34,452 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:34,870 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wq, in=4096, out=4096
2024-10-31:18:56:35,172 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wk, in=4096, out=1024
2024-10-31:18:56:35,302 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wv, in=4096, out=1024
2024-10-31:18:56:35,436 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wo, in=4096, out=4096
2024-10-31:18:56:35,753 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:36,203 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:36,662 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:37,140 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wq, in=4096, out=4096
2024-10-31:18:56:37,455 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wk, in=4096, out=1024
2024-10-31:18:56:37,587 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wv, in=4096, out=1024
2024-10-31:18:56:37,719 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wo, in=4096, out=4096
2024-10-31:18:56:38,032 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:38,475 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:38,887 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:39,195 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wq, in=4096, out=4096
2024-10-31:18:56:39,417 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wk, in=4096, out=1024
2024-10-31:18:56:39,435 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wv, in=4096, out=1024
2024-10-31:18:56:39,441 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wo, in=4096, out=4096
2024-10-31:18:56:39,591 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:39,865 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:40,149 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:40,577 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wq, in=4096, out=4096
2024-10-31:18:56:40,879 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wk, in=4096, out=1024
2024-10-31:18:56:41,011 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wv, in=4096, out=1024
2024-10-31:18:56:41,141 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wo, in=4096, out=4096
2024-10-31:18:56:41,286 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:41,523 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:41,861 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:42,330 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wq, in=4096, out=4096
2024-10-31:18:56:42,643 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wk, in=4096, out=1024
2024-10-31:18:56:42,771 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wv, in=4096, out=1024
2024-10-31:18:56:42,905 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wo, in=4096, out=4096
2024-10-31:18:56:43,207 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:43,635 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:44,077 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:44,513 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wq, in=4096, out=4096
2024-10-31:18:56:44,811 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wk, in=4096, out=1024
2024-10-31:18:56:44,924 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wv, in=4096, out=1024
2024-10-31:18:56:45,053 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wo, in=4096, out=4096
2024-10-31:18:56:45,358 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:45,724 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:46,153 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:46,406 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wq, in=4096, out=4096
2024-10-31:18:56:46,536 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wk, in=4096, out=1024
2024-10-31:18:56:46,564 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wv, in=4096, out=1024
2024-10-31:18:56:46,691 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wo, in=4096, out=4096
2024-10-31:18:56:46,989 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:47,419 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:47,833 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:48,085 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wq, in=4096, out=4096
2024-10-31:18:56:48,156 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wk, in=4096, out=1024
2024-10-31:18:56:48,161 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wv, in=4096, out=1024
2024-10-31:18:56:48,165 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wo, in=4096, out=4096
2024-10-31:18:56:48,265 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:48,566 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:48,919 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:49,349 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wq, in=4096, out=4096
2024-10-31:18:56:49,641 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wk, in=4096, out=1024
2024-10-31:18:56:49,768 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wv, in=4096, out=1024
2024-10-31:18:56:49,909 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wo, in=4096, out=4096
2024-10-31:18:56:50,220 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:50,653 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:51,102 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:51,541 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wq, in=4096, out=4096
2024-10-31:18:56:51,755 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wk, in=4096, out=1024
2024-10-31:18:56:51,760 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wv, in=4096, out=1024
2024-10-31:18:56:51,767 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wo, in=4096, out=4096
2024-10-31:18:56:51,870 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:52,169 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:52,499 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:52,790 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wq, in=4096, out=4096
2024-10-31:18:56:53,090 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wk, in=4096, out=1024
2024-10-31:18:56:53,217 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wv, in=4096, out=1024
2024-10-31:18:56:53,344 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wo, in=4096, out=4096
2024-10-31:18:56:53,628 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:54,047 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:54,460 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:54,892 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wq, in=4096, out=4096
2024-10-31:18:56:55,209 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wk, in=4096, out=1024
2024-10-31:18:56:55,343 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wv, in=4096, out=1024
2024-10-31:18:56:55,475 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wo, in=4096, out=4096
2024-10-31:18:56:55,792 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:56,234 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:56,685 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:57,123 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wq, in=4096, out=4096
2024-10-31:18:56:57,438 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wk, in=4096, out=1024
2024-10-31:18:56:57,572 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wv, in=4096, out=1024
2024-10-31:18:56:57,702 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wo, in=4096, out=4096
2024-10-31:18:56:57,998 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w1, in=4096, out=14336
2024-10-31:18:56:58,327 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w2, in=14336, out=4096
2024-10-31:18:56:58,760 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w3, in=4096, out=14336
2024-10-31:18:56:59,198 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wq, in=4096, out=4096
2024-10-31:18:56:59,523 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wk, in=4096, out=1024
2024-10-31:18:56:59,661 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wv, in=4096, out=1024
2024-10-31:18:56:59,792 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wo, in=4096, out=4096
2024-10-31:18:57:00,075 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:00,500 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:00,823 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:01,251 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wq, in=4096, out=4096
2024-10-31:18:57:01,561 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wk, in=4096, out=1024
2024-10-31:18:57:01,695 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wv, in=4096, out=1024
2024-10-31:18:57:01,835 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wo, in=4096, out=4096
2024-10-31:18:57:02,154 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:02,601 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:03,015 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:03,439 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wq, in=4096, out=4096
2024-10-31:18:57:03,762 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wk, in=4096, out=1024
2024-10-31:18:57:03,899 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wv, in=4096, out=1024
2024-10-31:18:57:04,029 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wo, in=4096, out=4096
2024-10-31:18:57:04,343 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:04,799 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:05,234 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:05,684 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wq, in=4096, out=4096
2024-10-31:18:57:06,009 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wk, in=4096, out=1024
2024-10-31:18:57:06,145 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wv, in=4096, out=1024
2024-10-31:18:57:06,279 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wo, in=4096, out=4096
2024-10-31:18:57:06,583 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:07,020 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:07,466 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:07,894 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wq, in=4096, out=4096
2024-10-31:18:57:08,202 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wk, in=4096, out=1024
2024-10-31:18:57:08,337 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wv, in=4096, out=1024
2024-10-31:18:57:08,469 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wo, in=4096, out=4096
2024-10-31:18:57:08,770 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:09,189 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:09,603 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:10,014 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wq, in=4096, out=4096
2024-10-31:18:57:10,301 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wk, in=4096, out=1024
2024-10-31:18:57:10,392 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wv, in=4096, out=1024
2024-10-31:18:57:10,402 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wo, in=4096, out=4096
2024-10-31:18:57:10,524 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:10,781 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:11,154 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:11,384 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wq, in=4096, out=4096
2024-10-31:18:57:11,454 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wk, in=4096, out=1024
2024-10-31:18:57:11,458 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wv, in=4096, out=1024
2024-10-31:18:57:11,467 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wo, in=4096, out=4096
2024-10-31:18:57:11,538 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:11,789 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:12,220 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:12,643 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wq, in=4096, out=4096
2024-10-31:18:57:12,928 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wk, in=4096, out=1024
2024-10-31:18:57:12,976 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wv, in=4096, out=1024
2024-10-31:18:57:13,015 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wo, in=4096, out=4096
2024-10-31:18:57:13,120 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:13,334 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:13,620 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:14,043 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wq, in=4096, out=4096
2024-10-31:18:57:14,348 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wk, in=4096, out=1024
2024-10-31:18:57:14,471 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wv, in=4096, out=1024
2024-10-31:18:57:14,597 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wo, in=4096, out=4096
2024-10-31:18:57:14,892 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:15,311 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:15,743 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:16,182 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wq, in=4096, out=4096
2024-10-31:18:57:16,500 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wk, in=4096, out=1024
2024-10-31:18:57:16,632 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wv, in=4096, out=1024
2024-10-31:18:57:16,763 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wo, in=4096, out=4096
2024-10-31:18:57:17,042 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:17,449 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:17,736 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:18,164 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wq, in=4096, out=4096
2024-10-31:18:57:18,471 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wk, in=4096, out=1024
2024-10-31:18:57:18,606 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wv, in=4096, out=1024
2024-10-31:18:57:18,741 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wo, in=4096, out=4096
2024-10-31:18:57:19,037 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:19,427 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:19,772 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:20,476 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wq, in=4096, out=4096
2024-10-31:18:57:20,949 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wk, in=4096, out=1024
2024-10-31:18:57:21,112 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wv, in=4096, out=1024
2024-10-31:18:57:21,268 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wo, in=4096, out=4096
2024-10-31:18:57:21,582 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:22,274 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:22,741 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:23,170 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wq, in=4096, out=4096
2024-10-31:18:57:23,472 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wk, in=4096, out=1024
2024-10-31:18:57:23,606 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wv, in=4096, out=1024
2024-10-31:18:57:23,717 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wo, in=4096, out=4096
2024-10-31:18:57:23,955 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w1, in=4096, out=14336
2024-10-31:18:57:24,199 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w2, in=14336, out=4096
2024-10-31:18:57:24,406 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w3, in=4096, out=14336
2024-10-31:18:57:24,718 INFO     [GPTQ.py:693] linear: model.output, in=4096, out=128256
Using device=cpu Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz
Loading model...
Time to load model: 0.11 seconds
Quantizing the model with: {'linear:int4': {'groupsize': 256}, 'precision': {'dtype': 'bfloat16'}, 'executor': {'accelerator': 'cpu'}}
Time to quantize model: 68.56 seconds
-----------------------------------------------------------
Once upon a time, in a small village nestled in the rolling hills of rural England, there lived a young girl named Sophie. Sophie was a happy, energetic child with a mop of curly brown hair and a smile that could light up a room. She loved nothing more than exploring the outdoors, playing with her friends, and getting into all sorts of mischief.
Sophie lived with her parents, Mary and John, in a cozy little cottage on the edge of the village. It was a beautiful place, surrounded by fields and woods, and a short walk from the village green. Sophie's parents were kind and loving people who always encouraged her to be her best self and explore her imagination.
One sunny afternoon, Sophie decided to venture out into the woods behind the village. She had been warned by her parents to be careful and stay on the path, but Sophie was always drawn to the unknown and the unexplored. She wandered deeper into the woods, her eyes scanning the underbrush for interesting leaves or sticks.
As she walked, Sophie came across a clearing surrounded by tall trees. In the center of the clearing stood an old, gnarled tree, its branches twisted and tangled in a way that seemed almost magical. Sophie felt drawn to the tree, and as she approached it, she noticed2024-10-31:19:05:07,687 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 1: 457.2395 sec total                 
Time to first token: 3.8511 sec with parallel prefill.                

      Total throughput: 0.5599 tokens/sec, 1.7861 s/token                 
First token throughput: 0.2597 tokens/sec, 3.8511 s/token                 
 Next token throughput: 0.5624 tokens/sec, 1.7780 s/token                     
2024-10-31:19:05:07,687 INFO     [generate.py:1173] 
Bandwidth achieved: 2.76 GB/s
2024-10-31:19:05:07,687 INFO     [generate.py:1177] *** This first iteration will include cold start effects for dynamic import, hardware caches. ***

========================================

Once upon a time, there was a young girl named Sarah who lived in a beautiful village surrounded by rolling hills and sparkling streams. She loved to explore the outdoors and dreamed of one day building a house of her own in a secluded spot, where she could watch the stars twinkling through a skylight above her bed.
Sarah's parents owned a small bakery in the village, famous for its delicious bread and sweet pastries. Sarah loved helping out at the bakery, learning the secrets of traditional recipes and preparing the day's offerings. One day, as she worked alongside her mother, a middle-aged woman with a warm smile, Sarah overheard her talking to a friend about a nearby plot of land that had just come up for sale. The friend was a real estate agent and promised to find a buyer at the right price.
Sarah watched her mother's face light up as she described her dream of building a house on the plot. But when the friend left, her mother's expression turned serious. Sarah's parents had been saving money for years, but they didn't have enough to put down a deposit on the plot. Still, their hope was that the price would go down, or a buyer would come who would sell quickly and give them a chance to build their dream home.

As the days2024-10-31:19:13:31,585 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 2: 503.8981 sec total                 
Time to first token: 3.9466 sec with parallel prefill.                

      Total throughput: 0.5080 tokens/sec, 1.9684 s/token                 
First token throughput: 0.2534 tokens/sec, 3.9466 s/token                 
 Next token throughput: 0.5100 tokens/sec, 1.9606 s/token                     
2024-10-31:19:13:31,586 INFO     [generate.py:1173] 
Bandwidth achieved: 2.50 GB/s

========================================

Once upon a time, in a small village surrounded by a dense forest, there lived a young girl named Lily. Lily was a gentle soul with a heart full of love and kindness. She spent most of her days exploring the forest, learning about the different plants and animals that lived there, and helping her mother with their household chores.
One day, while Lily was out collecting berries, she stumbled upon a small clearing with a beautiful old tree at its center. The tree was unlike any she had seen before – its branches were twisted and gnarled, and its trunk was covered in strange symbols that seemed to shimmer and glow in the sunlight.
Lily was both fascinated and intimidated by the tree. She had never seen anything like it before, and she wasn't sure if she should be near it. But as she stood there, a soft voice whispered in her ear, "Come closer, Lily. I have been waiting for you."
Lily spun around, looking for who had spoken, but there was no one in sight. She felt a shiver run down her spine, but she must have been mistaken, she thought. She approached the tree slowly, feeling a sense of wonder and curiosity.

As she drew closer, the symbols on the trunk began to glow even brighter, and Lily felt2024-10-31:19:18:56,952 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 3: 325.3664 sec total                 
Time to first token: 1.7988 sec with parallel prefill.                

      Total throughput: 0.7868 tokens/sec, 1.2710 s/token                 
First token throughput: 0.5559 tokens/sec, 1.7988 s/token                 
 Next token throughput: 0.7881 tokens/sec, 1.2689 s/token                     
2024-10-31:19:18:56,953 INFO     [generate.py:1173] 
Bandwidth achieved: 3.87 GB/s

========================================


      Average tokens/sec (total): 0.62                 
Average tokens/sec (first token): 0.36                 
Average tokens/sec (next tokens): 0.62 
                
Memory used: 0.00 GB
