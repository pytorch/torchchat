python3 torchchat.py export llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
python3 torchchat.py generate llama3.1 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py export llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
2024-10-31:14:00:36,175 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wq, in=4096, out=4096
2024-10-31:14:00:36,488 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wk, in=4096, out=1024
2024-10-31:14:00:36,632 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wv, in=4096, out=1024
2024-10-31:14:00:36,769 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wo, in=4096, out=4096
2024-10-31:14:00:37,067 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:37,488 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:37,924 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:38,374 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wq, in=4096, out=4096
2024-10-31:14:00:38,678 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wk, in=4096, out=1024
2024-10-31:14:00:38,812 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wv, in=4096, out=1024
2024-10-31:14:00:38,947 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wo, in=4096, out=4096
2024-10-31:14:00:39,257 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:39,710 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:40,162 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:40,602 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wq, in=4096, out=4096
2024-10-31:14:00:40,899 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wk, in=4096, out=1024
2024-10-31:14:00:41,033 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wv, in=4096, out=1024
2024-10-31:14:00:41,164 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wo, in=4096, out=4096
2024-10-31:14:00:41,477 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:41,938 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:42,407 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:42,886 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wq, in=4096, out=4096
2024-10-31:14:00:43,035 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wk, in=4096, out=1024
2024-10-31:14:00:43,061 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wv, in=4096, out=1024
2024-10-31:14:00:43,068 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wo, in=4096, out=4096
2024-10-31:14:00:43,182 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:43,495 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:43,783 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:44,212 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wq, in=4096, out=4096
2024-10-31:14:00:44,487 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wk, in=4096, out=1024
2024-10-31:14:00:44,597 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wv, in=4096, out=1024
2024-10-31:14:00:44,603 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wo, in=4096, out=4096
2024-10-31:14:00:44,726 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:45,033 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:45,221 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:45,507 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wq, in=4096, out=4096
2024-10-31:14:00:45,818 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wk, in=4096, out=1024
2024-10-31:14:00:45,952 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wv, in=4096, out=1024
2024-10-31:14:00:46,087 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wo, in=4096, out=4096
2024-10-31:14:00:46,384 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:46,811 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:47,237 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:47,697 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wq, in=4096, out=4096
2024-10-31:14:00:48,006 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wk, in=4096, out=1024
2024-10-31:14:00:48,142 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wv, in=4096, out=1024
2024-10-31:14:00:48,277 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wo, in=4096, out=4096
2024-10-31:14:00:48,581 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:48,984 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:49,342 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:49,565 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wq, in=4096, out=4096
2024-10-31:14:00:49,700 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wk, in=4096, out=1024
2024-10-31:14:00:49,776 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wv, in=4096, out=1024
2024-10-31:14:00:49,781 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wo, in=4096, out=4096
2024-10-31:14:00:49,868 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:50,142 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:50,568 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:51,010 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wq, in=4096, out=4096
2024-10-31:14:00:51,327 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wk, in=4096, out=1024
2024-10-31:14:00:51,459 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wv, in=4096, out=1024
2024-10-31:14:00:51,586 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wo, in=4096, out=4096
2024-10-31:14:00:51,896 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:52,327 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:52,771 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:53,225 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wq, in=4096, out=4096
2024-10-31:14:00:53,542 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wk, in=4096, out=1024
2024-10-31:14:00:53,675 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wv, in=4096, out=1024
2024-10-31:14:00:53,805 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wo, in=4096, out=4096
2024-10-31:14:00:54,121 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:54,572 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:55,028 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:55,466 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wq, in=4096, out=4096
2024-10-31:14:00:55,776 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wk, in=4096, out=1024
2024-10-31:14:00:55,849 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wv, in=4096, out=1024
2024-10-31:14:00:55,969 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wo, in=4096, out=4096
2024-10-31:14:00:56,110 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:56,569 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:57,053 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:57,528 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wq, in=4096, out=4096
2024-10-31:14:00:57,850 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wk, in=4096, out=1024
2024-10-31:14:00:57,983 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wv, in=4096, out=1024
2024-10-31:14:00:58,113 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wo, in=4096, out=4096
2024-10-31:14:00:58,448 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w1, in=4096, out=14336
2024-10-31:14:00:58,928 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w2, in=14336, out=4096
2024-10-31:14:00:59,405 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w3, in=4096, out=14336
2024-10-31:14:00:59,885 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wq, in=4096, out=4096
2024-10-31:14:01:00,223 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wk, in=4096, out=1024
2024-10-31:14:01:00,367 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wv, in=4096, out=1024
2024-10-31:14:01:00,499 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wo, in=4096, out=4096
2024-10-31:14:01:00,826 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:01,300 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:01,780 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:02,287 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wq, in=4096, out=4096
2024-10-31:14:01:02,626 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wk, in=4096, out=1024
2024-10-31:14:01:02,771 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wv, in=4096, out=1024
2024-10-31:14:01:02,909 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wo, in=4096, out=4096
2024-10-31:14:01:03,245 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:03,749 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:04,226 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:04,741 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wq, in=4096, out=4096
2024-10-31:14:01:05,070 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wk, in=4096, out=1024
2024-10-31:14:01:05,209 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wv, in=4096, out=1024
2024-10-31:14:01:05,333 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wo, in=4096, out=4096
2024-10-31:14:01:05,662 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:06,154 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:06,613 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:07,082 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wq, in=4096, out=4096
2024-10-31:14:01:07,414 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wk, in=4096, out=1024
2024-10-31:14:01:07,553 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wv, in=4096, out=1024
2024-10-31:14:01:07,686 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wo, in=4096, out=4096
2024-10-31:14:01:08,006 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:08,351 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:08,736 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:09,175 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wq, in=4096, out=4096
2024-10-31:14:01:09,490 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wk, in=4096, out=1024
2024-10-31:14:01:09,623 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wv, in=4096, out=1024
2024-10-31:14:01:09,768 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wo, in=4096, out=4096
2024-10-31:14:01:10,101 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:10,547 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:10,981 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:11,416 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wq, in=4096, out=4096
2024-10-31:14:01:11,719 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wk, in=4096, out=1024
2024-10-31:14:01:11,857 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wv, in=4096, out=1024
2024-10-31:14:01:11,985 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wo, in=4096, out=4096
2024-10-31:14:01:12,280 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:12,724 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:13,194 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:13,656 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wq, in=4096, out=4096
2024-10-31:14:01:13,970 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wk, in=4096, out=1024
2024-10-31:14:01:14,101 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wv, in=4096, out=1024
2024-10-31:14:01:14,245 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wo, in=4096, out=4096
2024-10-31:14:01:14,563 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:15,024 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:15,468 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:15,945 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wq, in=4096, out=4096
2024-10-31:14:01:16,270 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wk, in=4096, out=1024
2024-10-31:14:01:16,331 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wv, in=4096, out=1024
2024-10-31:14:01:16,358 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wo, in=4096, out=4096
2024-10-31:14:01:16,436 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:16,780 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:17,200 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:17,640 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wq, in=4096, out=4096
2024-10-31:14:01:17,953 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wk, in=4096, out=1024
2024-10-31:14:01:18,089 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wv, in=4096, out=1024
2024-10-31:14:01:18,224 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wo, in=4096, out=4096
2024-10-31:14:01:18,541 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:19,012 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:19,495 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:19,969 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wq, in=4096, out=4096
2024-10-31:14:01:20,295 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wk, in=4096, out=1024
2024-10-31:14:01:20,430 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wv, in=4096, out=1024
2024-10-31:14:01:20,563 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wo, in=4096, out=4096
2024-10-31:14:01:20,879 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:21,339 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:21,741 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:22,170 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wq, in=4096, out=4096
2024-10-31:14:01:22,485 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wk, in=4096, out=1024
2024-10-31:14:01:22,617 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wv, in=4096, out=1024
2024-10-31:14:01:22,746 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wo, in=4096, out=4096
2024-10-31:14:01:23,061 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:23,518 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:23,972 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:24,419 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wq, in=4096, out=4096
2024-10-31:14:01:24,738 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wk, in=4096, out=1024
2024-10-31:14:01:24,875 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wv, in=4096, out=1024
2024-10-31:14:01:25,010 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wo, in=4096, out=4096
2024-10-31:14:01:25,338 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:25,786 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:26,231 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:26,691 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wq, in=4096, out=4096
2024-10-31:14:01:27,005 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wk, in=4096, out=1024
2024-10-31:14:01:27,138 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wv, in=4096, out=1024
2024-10-31:14:01:27,266 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wo, in=4096, out=4096
2024-10-31:14:01:27,545 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:27,991 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:28,426 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:28,858 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wq, in=4096, out=4096
2024-10-31:14:01:29,168 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wk, in=4096, out=1024
2024-10-31:14:01:29,298 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wv, in=4096, out=1024
2024-10-31:14:01:29,436 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wo, in=4096, out=4096
2024-10-31:14:01:29,767 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:30,196 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:30,625 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:31,056 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wq, in=4096, out=4096
2024-10-31:14:01:31,381 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wk, in=4096, out=1024
2024-10-31:14:01:31,513 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wv, in=4096, out=1024
2024-10-31:14:01:31,649 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wo, in=4096, out=4096
2024-10-31:14:01:31,966 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:32,391 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:32,820 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:33,225 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wq, in=4096, out=4096
2024-10-31:14:01:33,542 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wk, in=4096, out=1024
2024-10-31:14:01:33,670 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wv, in=4096, out=1024
2024-10-31:14:01:33,807 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wo, in=4096, out=4096
2024-10-31:14:01:34,126 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:34,611 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:35,133 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:35,480 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wq, in=4096, out=4096
2024-10-31:14:01:35,817 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wk, in=4096, out=1024
2024-10-31:14:01:35,967 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wv, in=4096, out=1024
2024-10-31:14:01:36,102 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wo, in=4096, out=4096
2024-10-31:14:01:36,421 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:36,869 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:37,319 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:37,732 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wq, in=4096, out=4096
2024-10-31:14:01:38,051 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wk, in=4096, out=1024
2024-10-31:14:01:38,184 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wv, in=4096, out=1024
2024-10-31:14:01:38,322 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wo, in=4096, out=4096
2024-10-31:14:01:38,629 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:39,058 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:39,455 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:39,873 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wq, in=4096, out=4096
2024-10-31:14:01:40,175 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wk, in=4096, out=1024
2024-10-31:14:01:40,302 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wv, in=4096, out=1024
2024-10-31:14:01:40,436 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wo, in=4096, out=4096
2024-10-31:14:01:40,750 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:41,149 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:41,433 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:41,733 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wq, in=4096, out=4096
2024-10-31:14:01:42,068 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wk, in=4096, out=1024
2024-10-31:14:01:42,201 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wv, in=4096, out=1024
2024-10-31:14:01:42,337 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wo, in=4096, out=4096
2024-10-31:14:01:42,678 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w1, in=4096, out=14336
2024-10-31:14:01:43,145 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w2, in=14336, out=4096
2024-10-31:14:01:43,638 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w3, in=4096, out=14336
2024-10-31:14:01:44,128 INFO     [GPTQ.py:693] linear: model.output, in=4096, out=128256
W1031 14:01:50.694266 2746942 site-packages/torch/_export/__init__.py:225] +============================+
W1031 14:01:50.694721 2746942 site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |
W1031 14:01:50.694978 2746942 site-packages/torch/_export/__init__.py:227] +============================+
W1031 14:01:50.695175 2746942 site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.
Using device=cpu
Setting max_seq_length to 300 for DSO export.
Loading model...
Time to load model: 0.11 seconds
Quantizing the model with: {'linear:int4': {'groupsize': 256}, 'precision': {'dtype': 'bfloat16'}, 'executor': {'accelerator': 'cpu'}}
Time to quantize model: 74.34 seconds
-----------------------------------------------------------
Exporting model using AOT Inductor to /tmp/model34.so
The generated DSO model can be found at: /tmp/model34.so
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py generate llama3.1 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
Warning: checkpoint path ignored because an exported DSO or PTE path was specified
Warning: checkpoint path ignored because an exported DSO or PTE path was specified
Using device=cpu Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz
Loading model...
Time to load model: 0.11 seconds
-----------------------------------------------------------
Once upon a time, there was a beautiful small town surrounded by mountains and rivers. It was a peaceful and lovely place, where people lived happily together, enjoying the fresh air, beautiful landscapes and rich cultural heritage.
The town was called Willow Creek, and it was famous for its ancient willow trees that grew along the riverbank. The willows were not only beautiful but also very strong and resilient, able to withstand the strongest storms and the harshest winters.
One day, a young girl named Lily wandered into the woods near Willow Creek, looking for wildflowers to pick for her mother's birthday. As she walked along the riverbank, she stumbled upon an old, gnarled willow tree that seemed to be leaning precariously towards the river.
She remembered a legend that her grandmother used to tell her about the willows of Willow Creek. According to the legend, the willows had healing powers, and if you wished upon a star while standing under one of these trees, your wish would come true.
Being a curious and adventurous child, Lily decided to test the legend. She stood under the willow tree and made a wish. To her surprise, a shooting star streaked across the night sky, and Lily felt a strange and inexplicable sense of joy and peace.
2024-10-31:14:09:50,745 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 1: 297.5385 sec total                 
Time to first token: 13.3132 sec with sequential prefill.                

      Total throughput: 0.8604 tokens/sec, 1.1623 s/token                 
First token throughput: 0.0751 tokens/sec, 13.3132 s/token                 
 Next token throughput: 0.8972 tokens/sec, 1.1146 s/token                     
2024-10-31:14:09:50,746 INFO     [generate.py:1173] 
Bandwidth achieved: 13.82 GB/s
2024-10-31:14:09:50,746 INFO     [generate.py:1177] *** This first iteration will include cold start effects for dynamic import, hardware caches. ***

========================================

Once upon a time, there was a young girl named Maria who lived in a beautiful and peaceful village surrounded by lush green forests and a crystal-clear river. Maria loved to explore the outdoors and was fascinated by the mysteries of nature.
One day, while wandering through the forest, Maria came across a strange and enchanting sight. There was a magnificent tree, unlike any she had seen before, its trunk twisted and gnarled with age, its branches reaching high into the sky. The tree seemed to be glowing with a soft, ethereal light, and it seemed to be calling to Maria, drawing her closer.
As she approached the tree, Maria felt a sudden surge of excitement and wonder. She reached out and touched the trunk, and as she did, she felt a strange sensation wash over her. It was as if she was being transported to another world, a world where time and space were distorted and mysterious.
Maria found herself floating through the air, surrounded by a kaleidoscope of colors, each one shifting and changing in a way that was both familiar and yet completely alien. She felt a sense of weightlessness, as if she was floating on a cushion of air, and she was aware of the tree's energy, pulsating through her veins like a heartbeat.
Suddenly, Maria found2024-10-31:14:13:59,306 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 2: 248.5598 sec total                 
Time to first token: 0.8266 sec with sequential prefill.                

      Total throughput: 1.0299 tokens/sec, 0.9709 s/token                 
First token throughput: 1.2097 tokens/sec, 0.8266 s/token                 
 Next token throughput: 1.0293 tokens/sec, 0.9715 s/token                     
2024-10-31:14:13:59,306 INFO     [generate.py:1173] 
Bandwidth achieved: 16.54 GB/s

========================================

Once upon a time, in a small village nestled in the heart of a beautiful valley, there lived a young girl named Sophie. Sophie was a kind and gentle soul, with a heart full of love for all living things. She spent her days exploring the woods and fields around her village, collecting wild flowers and berries, and watching the animals that lived there.
One day, while out on one of her adventures, Sophie stumbled upon a hidden glade deep in the woods. It was a magical place, filled with sparkling fountains and fragrant flowers, and Sophie was drawn to its beauty like a magnet. As she wandered through the glade, she came across a tiny fairy perched on a toadstool. The fairy was no bigger than a thumb, with wings as delicate as a butterfly’s and hair as bright as a golden sunbeam.
“Who are you?” Sophie asked the fairy, her voice barely above a whisper.
“I am Luna, the guardian of this glade,” the fairy replied, her voice like the tinkling of a bell. “And you, dear Sophie, are a kindred spirit. I have been watching you, and I sense that you have a special gift – the gift of healing.”
Sophie was amazed. She had always known that she had2024-10-31:14:17:36,140 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 3: 216.8337 sec total                 
Time to first token: 4.6348 sec with sequential prefill.                

      Total throughput: 1.1806 tokens/sec, 0.8470 s/token                 
First token throughput: 0.2158 tokens/sec, 4.6348 s/token                 
 Next token throughput: 1.2017 tokens/sec, 0.8322 s/token                     
2024-10-31:14:17:36,140 INFO     [generate.py:1173] 
Bandwidth achieved: 18.96 GB/s

========================================


      Average tokens/sec (total): 1.02                 
Average tokens/sec (first token): 0.50                 
Average tokens/sec (next tokens): 1.04 
                
Memory used: 0.00 GB
