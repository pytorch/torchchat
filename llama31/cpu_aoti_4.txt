python3 torchchat.py export llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
python3 torchchat.py generate llama3.1 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py export llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
2024-10-31:21:03:23,970 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wq, in=4096, out=4096
2024-10-31:21:03:24,304 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wk, in=4096, out=1024
2024-10-31:21:03:24,444 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wv, in=4096, out=1024
2024-10-31:21:03:24,576 INFO     [GPTQ.py:693] linear: model.layers.0.attention.wo, in=4096, out=4096
2024-10-31:21:03:24,895 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:25,357 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:25,804 INFO     [GPTQ.py:693] linear: model.layers.0.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:26,241 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wq, in=4096, out=4096
2024-10-31:21:03:26,562 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wk, in=4096, out=1024
2024-10-31:21:03:26,707 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wv, in=4096, out=1024
2024-10-31:21:03:26,846 INFO     [GPTQ.py:693] linear: model.layers.1.attention.wo, in=4096, out=4096
2024-10-31:21:03:27,161 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:27,590 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:28,024 INFO     [GPTQ.py:693] linear: model.layers.1.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:28,424 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wq, in=4096, out=4096
2024-10-31:21:03:28,728 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wk, in=4096, out=1024
2024-10-31:21:03:28,858 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wv, in=4096, out=1024
2024-10-31:21:03:28,975 INFO     [GPTQ.py:693] linear: model.layers.2.attention.wo, in=4096, out=4096
2024-10-31:21:03:29,157 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:29,407 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:29,717 INFO     [GPTQ.py:693] linear: model.layers.2.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:29,993 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wq, in=4096, out=4096
2024-10-31:21:03:30,126 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wk, in=4096, out=1024
2024-10-31:21:03:30,164 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wv, in=4096, out=1024
2024-10-31:21:03:30,170 INFO     [GPTQ.py:693] linear: model.layers.3.attention.wo, in=4096, out=4096
2024-10-31:21:03:30,303 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:30,749 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:31,178 INFO     [GPTQ.py:693] linear: model.layers.3.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:31,510 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wq, in=4096, out=4096
2024-10-31:21:03:31,815 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wk, in=4096, out=1024
2024-10-31:21:03:31,940 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wv, in=4096, out=1024
2024-10-31:21:03:32,070 INFO     [GPTQ.py:693] linear: model.layers.4.attention.wo, in=4096, out=4096
2024-10-31:21:03:32,353 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:32,768 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:33,168 INFO     [GPTQ.py:693] linear: model.layers.4.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:33,603 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wq, in=4096, out=4096
2024-10-31:21:03:33,908 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wk, in=4096, out=1024
2024-10-31:21:03:34,039 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wv, in=4096, out=1024
2024-10-31:21:03:34,162 INFO     [GPTQ.py:693] linear: model.layers.5.attention.wo, in=4096, out=4096
2024-10-31:21:03:34,463 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:34,885 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:35,226 INFO     [GPTQ.py:693] linear: model.layers.5.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:35,534 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wq, in=4096, out=4096
2024-10-31:21:03:35,664 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wk, in=4096, out=1024
2024-10-31:21:03:35,668 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wv, in=4096, out=1024
2024-10-31:21:03:35,673 INFO     [GPTQ.py:693] linear: model.layers.6.attention.wo, in=4096, out=4096
2024-10-31:21:03:35,805 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:36,221 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:36,627 INFO     [GPTQ.py:693] linear: model.layers.6.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:37,019 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wq, in=4096, out=4096
2024-10-31:21:03:37,145 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wk, in=4096, out=1024
2024-10-31:21:03:37,184 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wv, in=4096, out=1024
2024-10-31:21:03:37,191 INFO     [GPTQ.py:693] linear: model.layers.7.attention.wo, in=4096, out=4096
2024-10-31:21:03:37,287 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:37,559 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:37,986 INFO     [GPTQ.py:693] linear: model.layers.7.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:38,407 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wq, in=4096, out=4096
2024-10-31:21:03:38,701 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wk, in=4096, out=1024
2024-10-31:21:03:38,836 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wv, in=4096, out=1024
2024-10-31:21:03:38,969 INFO     [GPTQ.py:693] linear: model.layers.8.attention.wo, in=4096, out=4096
2024-10-31:21:03:39,273 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:39,674 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:40,094 INFO     [GPTQ.py:693] linear: model.layers.8.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:40,500 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wq, in=4096, out=4096
2024-10-31:21:03:40,795 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wk, in=4096, out=1024
2024-10-31:21:03:40,920 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wv, in=4096, out=1024
2024-10-31:21:03:40,988 INFO     [GPTQ.py:693] linear: model.layers.9.attention.wo, in=4096, out=4096
2024-10-31:21:03:41,130 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:41,451 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:41,702 INFO     [GPTQ.py:693] linear: model.layers.9.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:41,994 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wq, in=4096, out=4096
2024-10-31:21:03:42,081 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wk, in=4096, out=1024
2024-10-31:21:03:42,087 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wv, in=4096, out=1024
2024-10-31:21:03:42,091 INFO     [GPTQ.py:693] linear: model.layers.10.attention.wo, in=4096, out=4096
2024-10-31:21:03:42,159 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:42,458 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:42,875 INFO     [GPTQ.py:693] linear: model.layers.10.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:43,269 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wq, in=4096, out=4096
2024-10-31:21:03:43,330 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wk, in=4096, out=1024
2024-10-31:21:03:43,335 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wv, in=4096, out=1024
2024-10-31:21:03:43,338 INFO     [GPTQ.py:693] linear: model.layers.11.attention.wo, in=4096, out=4096
2024-10-31:21:03:43,405 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:43,619 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:43,821 INFO     [GPTQ.py:693] linear: model.layers.11.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:44,113 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wq, in=4096, out=4096
2024-10-31:21:03:44,435 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wk, in=4096, out=1024
2024-10-31:21:03:44,570 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wv, in=4096, out=1024
2024-10-31:21:03:44,698 INFO     [GPTQ.py:693] linear: model.layers.12.attention.wo, in=4096, out=4096
2024-10-31:21:03:45,019 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:45,453 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:45,886 INFO     [GPTQ.py:693] linear: model.layers.12.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:46,300 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wq, in=4096, out=4096
2024-10-31:21:03:46,609 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wk, in=4096, out=1024
2024-10-31:21:03:46,744 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wv, in=4096, out=1024
2024-10-31:21:03:46,882 INFO     [GPTQ.py:693] linear: model.layers.13.attention.wo, in=4096, out=4096
2024-10-31:21:03:47,188 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:47,614 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:48,045 INFO     [GPTQ.py:693] linear: model.layers.13.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:48,445 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wq, in=4096, out=4096
2024-10-31:21:03:48,742 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wk, in=4096, out=1024
2024-10-31:21:03:48,871 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wv, in=4096, out=1024
2024-10-31:21:03:49,007 INFO     [GPTQ.py:693] linear: model.layers.14.attention.wo, in=4096, out=4096
2024-10-31:21:03:49,326 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:49,729 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:50,150 INFO     [GPTQ.py:693] linear: model.layers.14.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:50,573 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wq, in=4096, out=4096
2024-10-31:21:03:50,882 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wk, in=4096, out=1024
2024-10-31:21:03:51,016 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wv, in=4096, out=1024
2024-10-31:21:03:51,150 INFO     [GPTQ.py:693] linear: model.layers.15.attention.wo, in=4096, out=4096
2024-10-31:21:03:51,462 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:51,865 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:52,280 INFO     [GPTQ.py:693] linear: model.layers.15.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:52,690 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wq, in=4096, out=4096
2024-10-31:21:03:52,951 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wk, in=4096, out=1024
2024-10-31:21:03:53,068 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wv, in=4096, out=1024
2024-10-31:21:03:53,072 INFO     [GPTQ.py:693] linear: model.layers.16.attention.wo, in=4096, out=4096
2024-10-31:21:03:53,197 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:53,431 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:53,797 INFO     [GPTQ.py:693] linear: model.layers.16.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:54,047 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wq, in=4096, out=4096
2024-10-31:21:03:54,137 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wk, in=4096, out=1024
2024-10-31:21:03:54,141 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wv, in=4096, out=1024
2024-10-31:21:03:54,144 INFO     [GPTQ.py:693] linear: model.layers.17.attention.wo, in=4096, out=4096
2024-10-31:21:03:54,249 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:54,497 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:54,769 INFO     [GPTQ.py:693] linear: model.layers.17.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:55,186 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wq, in=4096, out=4096
2024-10-31:21:03:55,507 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wk, in=4096, out=1024
2024-10-31:21:03:55,515 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wv, in=4096, out=1024
2024-10-31:21:03:55,522 INFO     [GPTQ.py:693] linear: model.layers.18.attention.wo, in=4096, out=4096
2024-10-31:21:03:55,626 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:55,848 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:56,087 INFO     [GPTQ.py:693] linear: model.layers.18.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:56,387 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wq, in=4096, out=4096
2024-10-31:21:03:56,596 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wk, in=4096, out=1024
2024-10-31:21:03:56,730 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wv, in=4096, out=1024
2024-10-31:21:03:56,847 INFO     [GPTQ.py:693] linear: model.layers.19.attention.wo, in=4096, out=4096
2024-10-31:21:03:57,143 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:57,564 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w2, in=14336, out=4096
2024-10-31:21:03:58,000 INFO     [GPTQ.py:693] linear: model.layers.19.feed_forward.w3, in=4096, out=14336
2024-10-31:21:03:58,434 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wq, in=4096, out=4096
2024-10-31:21:03:58,748 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wk, in=4096, out=1024
2024-10-31:21:03:58,883 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wv, in=4096, out=1024
2024-10-31:21:03:59,020 INFO     [GPTQ.py:693] linear: model.layers.20.attention.wo, in=4096, out=4096
2024-10-31:21:03:59,332 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w1, in=4096, out=14336
2024-10-31:21:03:59,742 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:00,164 INFO     [GPTQ.py:693] linear: model.layers.20.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:00,488 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wq, in=4096, out=4096
2024-10-31:21:04:00,672 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wk, in=4096, out=1024
2024-10-31:21:04:00,676 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wv, in=4096, out=1024
2024-10-31:21:04:00,683 INFO     [GPTQ.py:693] linear: model.layers.21.attention.wo, in=4096, out=4096
2024-10-31:21:04:00,874 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:01,184 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:01,596 INFO     [GPTQ.py:693] linear: model.layers.21.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:02,041 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wq, in=4096, out=4096
2024-10-31:21:04:02,213 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wk, in=4096, out=1024
2024-10-31:21:04:02,228 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wv, in=4096, out=1024
2024-10-31:21:04:02,236 INFO     [GPTQ.py:693] linear: model.layers.22.attention.wo, in=4096, out=4096
2024-10-31:21:04:02,395 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:02,718 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:03,102 INFO     [GPTQ.py:693] linear: model.layers.22.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:03,510 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wq, in=4096, out=4096
2024-10-31:21:04:03,796 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wk, in=4096, out=1024
2024-10-31:21:04:03,919 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wv, in=4096, out=1024
2024-10-31:21:04:04,037 INFO     [GPTQ.py:693] linear: model.layers.23.attention.wo, in=4096, out=4096
2024-10-31:21:04:04,389 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:04,813 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:05,209 INFO     [GPTQ.py:693] linear: model.layers.23.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:05,627 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wq, in=4096, out=4096
2024-10-31:21:04:05,923 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wk, in=4096, out=1024
2024-10-31:21:04:06,054 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wv, in=4096, out=1024
2024-10-31:21:04:06,085 INFO     [GPTQ.py:693] linear: model.layers.24.attention.wo, in=4096, out=4096
2024-10-31:21:04:06,169 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:06,424 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:06,840 INFO     [GPTQ.py:693] linear: model.layers.24.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:07,248 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wq, in=4096, out=4096
2024-10-31:21:04:07,542 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wk, in=4096, out=1024
2024-10-31:21:04:07,675 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wv, in=4096, out=1024
2024-10-31:21:04:07,810 INFO     [GPTQ.py:693] linear: model.layers.25.attention.wo, in=4096, out=4096
2024-10-31:21:04:08,080 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:08,427 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:08,806 INFO     [GPTQ.py:693] linear: model.layers.25.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:09,203 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wq, in=4096, out=4096
2024-10-31:21:04:09,507 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wk, in=4096, out=1024
2024-10-31:21:04:09,632 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wv, in=4096, out=1024
2024-10-31:21:04:09,750 INFO     [GPTQ.py:693] linear: model.layers.26.attention.wo, in=4096, out=4096
2024-10-31:21:04:10,062 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:10,454 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:10,846 INFO     [GPTQ.py:693] linear: model.layers.26.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:11,237 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wq, in=4096, out=4096
2024-10-31:21:04:11,521 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wk, in=4096, out=1024
2024-10-31:21:04:11,653 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wv, in=4096, out=1024
2024-10-31:21:04:11,785 INFO     [GPTQ.py:693] linear: model.layers.27.attention.wo, in=4096, out=4096
2024-10-31:21:04:11,968 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:12,235 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:12,485 INFO     [GPTQ.py:693] linear: model.layers.27.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:12,825 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wq, in=4096, out=4096
2024-10-31:21:04:13,076 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wk, in=4096, out=1024
2024-10-31:21:04:13,082 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wv, in=4096, out=1024
2024-10-31:21:04:13,086 INFO     [GPTQ.py:693] linear: model.layers.28.attention.wo, in=4096, out=4096
2024-10-31:21:04:13,159 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:13,522 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:13,924 INFO     [GPTQ.py:693] linear: model.layers.28.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:14,322 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wq, in=4096, out=4096
2024-10-31:21:04:14,461 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wk, in=4096, out=1024
2024-10-31:21:04:14,466 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wv, in=4096, out=1024
2024-10-31:21:04:14,470 INFO     [GPTQ.py:693] linear: model.layers.29.attention.wo, in=4096, out=4096
2024-10-31:21:04:14,532 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:14,729 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:15,051 INFO     [GPTQ.py:693] linear: model.layers.29.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:15,324 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wq, in=4096, out=4096
2024-10-31:21:04:15,627 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wk, in=4096, out=1024
2024-10-31:21:04:15,759 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wv, in=4096, out=1024
2024-10-31:21:04:15,896 INFO     [GPTQ.py:693] linear: model.layers.30.attention.wo, in=4096, out=4096
2024-10-31:21:04:16,210 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:16,638 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:17,063 INFO     [GPTQ.py:693] linear: model.layers.30.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:17,469 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wq, in=4096, out=4096
2024-10-31:21:04:17,774 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wk, in=4096, out=1024
2024-10-31:21:04:17,899 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wv, in=4096, out=1024
2024-10-31:21:04:18,030 INFO     [GPTQ.py:693] linear: model.layers.31.attention.wo, in=4096, out=4096
2024-10-31:21:04:18,319 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w1, in=4096, out=14336
2024-10-31:21:04:18,702 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w2, in=14336, out=4096
2024-10-31:21:04:18,880 INFO     [GPTQ.py:693] linear: model.layers.31.feed_forward.w3, in=4096, out=14336
2024-10-31:21:04:19,157 INFO     [GPTQ.py:693] linear: model.output, in=4096, out=128256
W1031 21:04:26.519690 2005032 site-packages/torch/_export/__init__.py:225] +============================+
W1031 21:04:26.520187 2005032 site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |
W1031 21:04:26.520403 2005032 site-packages/torch/_export/__init__.py:227] +============================+
W1031 21:04:26.520590 2005032 site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.
Using device=cpu
Setting max_seq_length to 300 for DSO export.
Loading model...
Time to load model: 0.13 seconds
Quantizing the model with: {'linear:int4': {'groupsize': 256}, 'precision': {'dtype': 'bfloat16'}, 'executor': {'accelerator': 'cpu'}}
Time to quantize model: 61.74 seconds
-----------------------------------------------------------
Exporting model using AOT Inductor to /tmp/model34.so
The generated DSO model can be found at: /tmp/model34.so
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py generate llama3.1 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
Warning: checkpoint path ignored because an exported DSO or PTE path was specified
Warning: checkpoint path ignored because an exported DSO or PTE path was specified
Using device=cpu Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz
Loading model...
Time to load model: 0.11 seconds
-----------------------------------------------------------
Once upon a time, there was a handsome and intelligent boy named Joe. He had a passion for photography and spent most of his days taking pictures of everything that caught his eye. Joe was shy and introverted, but he had a kind heart and was always willing to lend a helping hand to those in need.
One day, Joe decided to visit his friend Emma, who lived in a small village on the outskirts of town. Emma was a free-spirited girl who loved music and dance. She had a passion for playing the guitar and writing songs, and she often spent her days practicing and performing for her friends.
As soon as Joe arrived in the village, he was struck by the beauty of the surroundings. The village was surrounded by lush green forests and rolling hills, and the air was filled with the sweet scent of blooming flowers. Joe took his camera and began to capture the beauty of the village, taking pictures of everything from the thatched roofs of the cottages to the sparkling streams that ran through the village.
Emma was delighted to see Joe and immediately invited him to join her for a walk through the village. As they strolled through the village, Emma pointed out all of the hidden gems that she had discovered during her time there. She took Joe to a secret waterfall that2024-10-31:21:10:49,861 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 1: 275.5374 sec total                 
Time to first token: 15.6001 sec with sequential prefill.                

      Total throughput: 0.9291 tokens/sec, 1.0763 s/token                 
First token throughput: 0.0641 tokens/sec, 15.6001 s/token                 
 Next token throughput: 0.9810 tokens/sec, 1.0194 s/token                     
2024-10-31:21:10:49,861 INFO     [generate.py:1173] 
Bandwidth achieved: 14.92 GB/s
2024-10-31:21:10:49,861 INFO     [generate.py:1177] *** This first iteration will include cold start effects for dynamic import, hardware caches. ***

========================================

Once upon a time, in a world very much like our own, there was a young woman named Lily who had always been fascinated by the art of cooking. She had grown up watching her mother and grandmother cook, and had even helped with some simple recipes as a child. As she got older, Lily became more and more interested in the culinary arts, and decided to attend a prestigious cooking school in a nearby city.
Lily was excited to learn all the latest techniques and ingredients, and was eager to share her newfound knowledge with her family and friends. She worked hard, pouring all of her energy into her studies, and was soon cooking like a pro. With her newfound skills, Lily landed a job at a trendy new restaurant in the city, where she quickly became a rising star in the culinary world.

But Lily's success was not without its challenges. She worked long hours for low pay, often having to sacrifice sleep in order to finish her shifts on time. She also dealt with the constant pressure of pleasing the picky restaurant owners, who were always demanding new and innovative dishes. And worst of all, Lily was torn between her love of cooking and her desire for a more balanced life.

Despite these struggles, Lily persevered, driven by her passion for cooking and her desire to make a2024-10-31:21:14:47,486 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 2: 237.6246 sec total                 
Time to first token: 2.7085 sec with sequential prefill.                

      Total throughput: 1.0773 tokens/sec, 0.9282 s/token                 
First token throughput: 0.3692 tokens/sec, 2.7085 s/token                 
 Next token throughput: 1.0855 tokens/sec, 0.9212 s/token                     
2024-10-31:21:14:47,486 INFO     [generate.py:1173] 
Bandwidth achieved: 17.30 GB/s

========================================

Once upon a time, there was an unfulfilled dream for a young woman named Lily. She had grown up on the streets of a city called Corvina, where she had learned to rely heavily on her own wits and brawn. She had always dreamed of opening her own restaurant, a place where she could serve mouth-watering dishes and provide a sense of warmth and welcome to the people of the city.
Lily had been working hard for years, saving up whatever money she could, but the truth was that it seemed beyond her reach. She had come to a point where she felt like giving up, convinced that her dream was something that only others could achieve.

One day, an unexpected opportunity presented itself. Lily was approached by a group of investors who were interested in opening a restaurant franchise in Corvina. They wanted a woman with a strong social media presence and a passion for food to be the face of their franchise. Lily's eyes lit up with excitement as she was interviewed, and she was offered a chance to run her own restaurant as part of their franchise.

As she sat in the small, cramped apartment that had been her home for so long, Lily couldn't believe that she had been given the chance to live her dream. She had always thought that it was2024-10-31:21:17:48,744 INFO     [generate.py:1162] 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                
Generated 255 tokens                 
Time for inference 3: 181.2574 sec total                 
Time to first token: 1.7974 sec with sequential prefill.                

      Total throughput: 1.4124 tokens/sec, 0.7080 s/token                 
First token throughput: 0.5563 tokens/sec, 1.7974 s/token                 
 Next token throughput: 1.4209 tokens/sec, 0.7038 s/token                     
2024-10-31:21:17:48,744 INFO     [generate.py:1173] 
Bandwidth achieved: 22.68 GB/s

========================================


      Average tokens/sec (total): 1.14                 
Average tokens/sec (first token): 0.33                 
Average tokens/sec (next tokens): 1.16 
                
Memory used: 0.00 GB
