# torchchat Distributed Config.toml

[job]
dump_folder = "./outputs"
description = "Llama 3 distributed inference"
use_for_integration_test = true

[profiling]
enable_profiling = false
save_traces_folder = "profile_trace"
profile_freq = 10
enable_memory_snapshot = false
save_memory_snapshot_folder = "memory_snapshot"

[metrics]
enable_color_printing = true
enable_tensorboard = true
save_tb_folder = "tb"

[model]
name = "llama3"
flavor = "8B"
tokenizer_path = "./test/assets/test_tiktoken.model"

[inference]
batch_size = 8
seq_len = 2048
data_parallel_degree = -1
tensor_parallel_degree = 1
fp8_linear = ""
compile = false
pipeline_parallel_degree = 1
