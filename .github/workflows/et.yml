name: Compile main

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

jobs:
  run-tinystories:
    strategy:
      matrix:
        runner: [32-core-ubuntu]
    runs-on: ${{matrix.runner}}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.11
      - name: Print machine info
        run: |
          uname -a
          if [ $(uname -s) == Darwin ]; then
            sysctl machdep.cpu.brand_string
            sysctl machdep.cpu.core_count
          fi
      - name: Install requirements
        run: |
          echo "Intalling pip packages"
          pip install wheel
          pip install cmake
          pip install ninja
          pip install zstd
          pip install -r requirements.txt

          echo "Executorch: cloning"
          mkdir etorch
          cd etorch
          git clone https://github.com/pytorch/executorch.git
          cd executorch
          echo "Inside: ${PWD}"

          echo "Executorch: submodule update"
          git submodule sync
          git submodule update --init

          echo "Executorch: installing python interface"
          ./install_requirements.sh --pybind xnnpack

          cd ../..
          echo "Inside: ${PWD}"
      - name: Download checkpoints
        run: |
          mkdir -p checkpoints/stories15M
          pushd checkpoints/stories15M
          wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.pt
          wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.model
          wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin
          popd

          mkdir gguf_files
          export GGUF_PATH=gguf_files/TinyLlama-1.1B-openorca.Q4_0.gguf
          export GGUF_TOKENIZER_PATH=gguf_files/tokenizer.model
          wget -O ${GGUF_PATH} "https://huggingface.co/TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF/resolve/main/tinyllama-1.1b-1t-openorca.Q4_0.gguf?download=true"
          wget -O ${GGUF_TOKENIZER_PATH} https://github.com/karpathy/llama2.c/raw/master/tokenizer.model

      - name: Run inference
        run: |
          export MODEL_PATH=${PWD}/checkpoints/stories15M/stories15M.pt
          export MODEL_NAME=stories15M

          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 > ${PWD}/output_eager
          cat ${PWD}/output_eager

          python export.py --checkpoint-path ${MODEL_PATH} --output-pte-path ${PWD}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${PWD}/${MODEL_NAME}.pte  > ${PWD}/output_et
          cat ${PWD}/output_et

          echo "Tests complete."

      - name: Run inference
        run: |
          export MODEL_PATH=checkpoints/stories15M/stories15M.pt
          export MODEL_NAME=stories15M
          export MODEL_DIR=/tmp
          python export.py --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          cat ./output_et

          echo "******************************************"
          echo "******* Emb: channel-wise quantized ******"
          echo "******************************************"
          python export.py --quant '{"embedding" : {"bitwidth": 8, "groupsize": 0}}' --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          cat ./output_et

          echo "******************************************"
          echo "******** Emb: group-wise quantized *******"
          echo "******************************************"
          python export.py --quant '{"embedding" : {"bitwidth": 8, "groupsize": 8}}' --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          cat ./output_et

          echo "******************************************"
          echo "******* INT8 channel-wise quantized ******"
          echo "******************************************"
          python export.py --quant '{"linear:int8" : {"bitwidth": 8, "groupsize": 0}}' --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          cat ./output_et

          echo "******************************************"
          echo "******** INT8 group-wise quantized *******"
          echo "******************************************"
          python export.py --quant '{"linear:int8" : {"bitwidth": 8, "groupsize": 8}}' --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          cat ./output_et

          echo "******************************************"
          echo "******** INT4 group-wise quantized *******"
          echo "******************************************"
          # python export.py --quant '{"linear:int4" : {"groupsize": 32}}' --checkpoint-path ${MODEL_PATH} --output-pte-path ${MODEL_DIR}/${MODEL_NAME}.pte
          # python generate.py --checkpoint-path ${MODEL_PATH} --temperature 0 --pte-path ${MODEL_DIR}/${MODEL_NAME}.pte  > ./output_et
          # cat ./output_et

          echo "tests complete"
          echo "******************************************"

      - name: Run GGUF export + inference
        run: |
          export GGUF_PATH=gguf_files/TinyLlama-1.1B-openorca.Q4_0.gguf
          export GGUF_TOKENIZER_PATH=gguf_files/tokenizer.model

          python torchchat.py export --gguf-path ${GGUF_PATH} --output-pte-path ${PWD}/${MODEL_NAME}.pte
          python torchchat.py generate --gguf-path ${GGUF_PATH} --pte-path ${PWD}/${MODEL_NAME}.pte --tokenizer-path ${GGUF_TOKENIZER_PATH} --temperature 0 --max-new-tokens 20 > ${PWD}/output_et
          cat ${PWD}/output_et

          echo "Tests complete."
